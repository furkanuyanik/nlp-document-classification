# -*- coding: utf-8 -*-
"""NLP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14QmW0TzLgA7ZojfKwYG_RjGuwX9m11fI

# Libraries
"""

import glob
import os
import nltk
from nltk.stem import PorterStemmer 
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.corpus import wordnet
nltk.download('stopwords');
nltk.download('punkt');
nltk.download('wordnet')

"""# Functions

File IO Functions
"""

# Read text files
def readFiles(file_names):
  raw_articles = [];

  for file_name in file_names:
    f = open(file_name, "r");
    if f.mode == 'r':
      try:
        raw_articles.append({file_name: f.read()});
        preprocessed_file_names.append(file_name)
      except: 
        not_preprocessed_file_names.append(file_name)

  return raw_articles;

# Create directory by path
def create_directory(directory_name):
  if not os.path.exists(directory_name):
    os.mkdir(directory_name)
    print("Directory ", directory_name,  " created.")
  else:    
    print("Directory ", directory_name,  " already exists.")

# Write content in the text file
def writeFile(file_name, content):
  f = open(file_name, "w+")
  f.write(content)
  f.close()

# Get text file names
def getFileNames(path):
  return glob.glob(path + "/*.txt");

# Get stop-words
def getStopWords():
  return set(stopwords.words('english'));

"""Preprocessing Functions"""

# Get splitted words
def getTokens(text):
  return word_tokenize(text.lower());

# Remove stop-word 
def removeStopWords(text):
  temp = [w for w in getTokens(text) if not w in stopwords.words()]
  return " ".join(temp)

# Remove duplicated words 
def removeDuplicatedWords(text):
    temp = []
    [temp.append(' '+w+' ') for w in getTokens(text) if (' '+w+' ') not in temp]
    return " ".join(getTokens("".join(temp)))

# Get root of words
def getStemmingWords(text):
  temp = [];
  porter_stemmer = PorterStemmer() 
  for w in getTokens(text): 
    temp.append(porter_stemmer.stem(w));
  return " ".join(temp)

# Remove special characters in the words
def removeSpecialCharacters(text):
  temp = list(text) 
  for x in range(len(text)):
    if text[x] != ' ' and (text[x].isalnum() == False or text[x].isdigit() == True):
        temp[x] = ' '

  return "".join(temp)

def removeSynonymWords(text):
  temp = []
  for word in getTokens(text):
    for synset in wordnet.synsets(word):
      for lemma in synset.lemma_names():
          if lemma.lower() != word and lemma.find("_") == -1 and not lemma in temp and text.find(' ' + lemma + ' ') != -1:
            text = text.replace(' ' + lemma + ' ',' ')
            temp.append(lemma)
            temp.append(word)
  return text

"""# Variables"""

stop_words =  getStopWords();

preprocessed_file_names = []
not_preprocessed_file_names = []

train_education_file_names = getFileNames("train/raw/education")
train_sport_file_names = getFileNames("train/raw/sport")
test_education_file_names = getFileNames("test/raw/education")
test_sport_file_names = getFileNames("test/raw/sport")

train_education_contents = readFiles(train_education_file_names)
train_sport_contents = readFiles(train_sport_file_names)
test_education_contents = readFiles(test_education_file_names)
test_sport_contents = readFiles(test_sport_file_names)

preprocessed_directory_name = "train/preprocessed"
preprocessed_education_directory_name = "train/preprocessed/education"
preprocessed_sport_directory_name = "train/preprocessed/sport"
preprocessed_test_directory_name = "test/preprocessed"
preprocessed_test_education_directory_name = "test/preprocessed/education"
preprocessed_test_sport_directory_name = "test/preprocessed/sport"

"""# Preprocess"""

# Create directory to save for preprocessed files
create_directory(preprocessed_directory_name)
create_directory(preprocessed_education_directory_name)
create_directory(preprocessed_sport_directory_name)

# Preprocess for education category
for x in range(len(train_education_contents)):
  for k,v in train_education_contents[x].items():
    text_for_education = removeSpecialCharacters(v)
    text_for_education = removeStopWords(text_for_education)
    #text_for_education = removeDuplicatedWords(text_for_education)
    text_for_education = removeSynonymWords(text_for_education)
    text_for_education = getStemmingWords(text_for_education)
    #text_for_education = removeDuplicatedWords(text_for_education)
    file_path = preprocessed_education_directory_name + '/' + k[k.rindex('/') + 1:]
    writeFile(file_path, text_for_education)

print("Prepocessed completed for education category.")

# Preprocess for sport category
for x in range(len(train_sport_contents)):
  for k,v in train_sport_contents[x].items():
    text_for_sport = removeSpecialCharacters(v)
    text_for_sport = removeStopWords(text_for_sport)
    #text_for_sport = removeDuplicatedWords(text_for_sport)
    text_for_sport = removeSynonymWords(text_for_sport)
    text_for_sport = getStemmingWords(text_for_sport)
    #text_for_sport = removeDuplicatedWords(text_for_sport)
    file_path = preprocessed_sport_directory_name + '/' + k[k.rindex('/') + 1:]
    writeFile(file_path, text_for_sport)

print("Prepocessed completed for sport category.")

# Create directory to save for preprocessed files
create_directory(preprocessed_test_directory_name)
create_directory(preprocessed_test_education_directory_name)
create_directory(preprocessed_test_sport_directory_name)

# Preprocess for education category
for x in range(len(test_education_contents)):
  for k,v in test_education_contents[x].items():
    text_for_education = removeSpecialCharacters(v)
    text_for_education = removeStopWords(text_for_education)
    #text_for_education = removeDuplicatedWords(text_for_education)
    text_for_education = removeSynonymWords(text_for_education)
    text_for_education = getStemmingWords(text_for_education)
    #text_for_education = removeDuplicatedWords(text_for_education)
    file_path = preprocessed_test_education_directory_name + '/' + k[k.rindex('/') + 1:]
    writeFile(file_path, text_for_education)

print("Prepocessed completed for education category.")

# Preprocess for sport category
for x in range(len(test_sport_contents)):
  for k,v in test_sport_contents[x].items():
    text_for_sport = removeSpecialCharacters(v)
    text_for_sport = removeStopWords(text_for_sport)
    #text_for_sport = removeDuplicatedWords(text_for_sport)
    text_for_sport = removeSynonymWords(text_for_sport)
    text_for_sport = getStemmingWords(text_for_sport)
    #text_for_sport = removeDuplicatedWords(text_for_sport)
    file_path = preprocessed_test_sport_directory_name + '/' + k[k.rindex('/') + 1:]
    writeFile(file_path, text_for_sport)

print("Prepocessed completed for sport category.")

# Download preprocessed files.
!zip -r train/preprocessed.zip train/preprocessed

from google.colab import files
files.download("train/preprocessed.zip")

print("Preprocessed text files downloaded.")

# Download preprocessed files.
!zip -r test/preprocessed.zip test/preprocessed

from google.colab import files
files.download("test/preprocessed.zip")

print("Preprocessed text files downloaded.")

"""# Methods

Prepaire for calculate probability
"""

import nltk
from nltk import ConditionalFreqDist

preprocessed_education_file_names = getFileNames(preprocessed_education_directory_name)
preprocessed_sport_file_names = getFileNames(preprocessed_sport_directory_name)
preprocessed_test_education_file_names = getFileNames(preprocessed_test_education_directory_name)
preprocessed_test_sport_file_names = getFileNames(preprocessed_test_sport_directory_name)

preprocessed_education_contents = readFiles(preprocessed_education_file_names)
preprocessed_sport_contents = readFiles(preprocessed_sport_file_names)
preprocessed_test_education_contents = readFiles(preprocessed_test_education_file_names)
preprocessed_test_sport_contents = readFiles(preprocessed_test_sport_file_names)

# Required for TF and TF-IDF
bagOfWords = {}

education_total_text = ''
for x in range(len(preprocessed_education_contents)):
  for k, v in preprocessed_education_contents[x].items():
    education_total_text = education_total_text + ' ' + v
    bagOfWords.update({k : v.split(' ')})

sport_total_text = ''
for x in range(len(preprocessed_sport_contents)):
  for k, v in preprocessed_sport_contents[x].items():
    sport_total_text = sport_total_text + ' ' + v
    bagOfWords.update({k : v.split(' ')})

total_text = education_total_text + ' ' + sport_total_text

total_for_education = len(getTokens(education_total_text))
total_for_sport = len(getTokens(sport_total_text))

bigrams_for_education = nltk.bigrams(getTokens(education_total_text))
bigrams_for_sport = nltk.bigrams(getTokens(sport_total_text))

trigrams_for_education = nltk.trigrams(getTokens(education_total_text))
trigrams_for_sport = nltk.trigrams(getTokens(sport_total_text))

fd_unigrams_for_education = nltk.FreqDist(getTokens(education_total_text))
fd_unigrams_for_sport = nltk.FreqDist(getTokens(sport_total_text))

cfd_bigrams_for_education = nltk.ConditionalFreqDist(bigrams_for_education) 
cfd_bigrams_for_sport = nltk.ConditionalFreqDist(bigrams_for_sport) 

cfd_trigrams_for_education = nltk.ConditionalFreqDist(((w2, (w0, w1)) for w0, w1, w2 in trigrams_for_education))
cfd_trigrams_for_sport = nltk.ConditionalFreqDist(((w2, (w0, w1)) for w0, w1, w2 in trigrams_for_sport))

"""Probability Functions"""

# For 1-gram; count(w1) + 1 / V (1-gram total count)
# For 2-gram; count(w2|w1) + 1 / count(w1) + V (2-gram total count)
# For 3-gram; count(w3|w1,w2) + 1 / count(w1,w2) + V (3-gram total count)
def laplaceSmoothing(total_count, word_n_count, word_n1_count = 0):
  return (word_n_count + 1) / (word_n1_count + total_count)

def computeTF(wordDict, bagOfWords, wordsCount):
    tfDict = {}
    bagOfWordsCount = len(bagOfWords)
    for word, count in wordDict.items():
        tfDict[word] = (count + 1) / (float(bagOfWordsCount) + wordsCount)
    return tfDict

def computeIDF(documents):
    import math
    N = len(documents)
    
    idfDict = dict.fromkeys(documents[0].keys(), 0)
    for document in documents:
        for word, val in document.items():
            if val > 0:
                idfDict[word] += 1
    
    for word, val in idfDict.items():
        idfDict[word] = math.log((N+1) / (float(val)+1)) + 1
    return idfDict

def computeTFIDF(tfBagOfWords, idfs):
    tfidf = {}
    for word, val in tfBagOfWords.items():
        tfidf[word] = val * idfs[word]
    return tfidf

"""Compute 1-gram Probability for Education category"""

education_unigrams_probability = {}

for k, v in fd_unigrams_for_education.items():
  education_unigrams_probability.update({'P(' + k + ')': laplaceSmoothing(len(fd_unigrams_for_education.items()), v)})

education_unigrams_probability = {k: v for k, v in sorted(education_unigrams_probability.items(), key=lambda item: item[1], reverse=True)}

f = open("train/preprocessed/education_unigrams_probabilities.txt","w+")
for k, v in education_unigrams_probability.items():
  f.write(k+ ": %f\r\n" % (v))
f.close()

"""Compute 2-gram Probability for Education category"""

education_bigrams_probability = {}

for key, value in cfd_bigrams_for_education.items():
  for sub_key, sub_value in value.items():
    education_bigrams_probability.update({'P(' + key + ', ' + sub_key + ')': laplaceSmoothing(len(cfd_bigrams_for_education.items()), cfd_bigrams_for_education[key][sub_key], fd_unigrams_for_education[sub_key])})

education_bigrams_probability = {k: v for k, v in sorted(education_bigrams_probability.items(), key=lambda item: item[1], reverse=True)}

f = open("train/preprocessed/education_bigrams_probabilities.txt","w+")
for k, v in education_bigrams_probability.items():
  f.write(k+ ": %f\r\n" % (v))
f.close()

"""Compute 3-gram Probability for Education category"""

education_trigrams_probability = {}

for key, value in cfd_trigrams_for_education.items():
  for sub_key, sub_value in value.items():
    education_trigrams_probability.update({'P(' + key + ', ' + sub_key[0] + ', '+sub_key[1]+ ')' : laplaceSmoothing(len(cfd_trigrams_for_education.items()), cfd_trigrams_for_education[key][sub_key], cfd_bigrams_for_education[sub_key[1]][sub_key[0]])})

education_trigrams_probability = {k: v for k, v in sorted(education_trigrams_probability.items(), key=lambda item: item[1], reverse=True)}

f = open("train/preprocessed/education_trigrams_probabilities.txt","w+")
for k, v in education_trigrams_probability.items():
  f.write(k+ ": %f\r\n" % (v))
f.close()

"""Compute 1-gram Probability for Sport category"""

sport_unigrams_probability = {}

for k, v in fd_unigrams_for_sport.items():
  sport_unigrams_probability.update({'P(' + k + ')': laplaceSmoothing(len(fd_unigrams_for_sport.items()), v)})

sport_unigrams_probability = {k: v for k, v in sorted(sport_unigrams_probability.items(), key=lambda item: item[1], reverse=True)}

f = open("train/preprocessed/sport_unigrams_probabilities.txt","w+")
for k, v in sport_unigrams_probability.items():
  f.write(k+ ": %f\r\n" % (v))
f.close()

"""Compute 2-gram Probability for Sport category"""

sport_bigrams_probability = {}

for key, value in cfd_bigrams_for_sport.items():
  for sub_key, sub_value in value.items():
    sport_bigrams_probability.update({'P(' + key + ', ' + sub_key + ')': laplaceSmoothing(len(cfd_bigrams_for_sport.items()), cfd_bigrams_for_sport[key][sub_key], fd_unigrams_for_sport[sub_key])})

sport_bigrams_probability = {k: v for k, v in sorted(sport_bigrams_probability.items(), key=lambda item: item[1], reverse=True)}

f = open("train/preprocessed/sport_bigrams_probabilities.txt","w+")
for k, v in sport_bigrams_probability.items():
  f.write(k+ ": %f\r\n" % (v))
f.close()

"""Compute 3-gram Probability for Sport category"""

sport_trigrams_probability = {}

for key, value in cfd_trigrams_for_sport.items():
  for sub_key, sub_value in value.items():
    sport_trigrams_probability.update({'P(' + key + ', ' + sub_key[0] + ', '+sub_key[1]+ ')' : laplaceSmoothing(len(cfd_trigrams_for_sport.items()), cfd_trigrams_for_sport[key][sub_key], cfd_bigrams_for_sport[sub_key[1]][sub_key[0]])})

sport_trigrams_probability = {k: v for k, v in sorted(sport_trigrams_probability.items(), key=lambda item: item[1], reverse=True)}

f = open("train/preprocessed/sport_trigrams_probabilities.txt","w+")
for k, v in sport_trigrams_probability.items():
  f.write(k+ ": %f\r\n" % (v))
f.close()

"""Compute TF and TF-IDF Probability"""

uniqueWords = getTokens(removeDuplicatedWords(total_text))

# For Artificial neural network (education: 1, sport: 0)
special_class = []

numOfWords = {}
for x in range(len(preprocessed_education_contents)):
  for k, v in preprocessed_education_contents[x].items():
    numOfWordsForArticle = dict.fromkeys(uniqueWords, 0)
    for word in v.split(' '):
      numOfWordsForArticle[word] += 1
    numOfWords.update({k: numOfWordsForArticle})
    special_class.append(1)

for x in range(len(preprocessed_sport_contents)):
  for k, v in preprocessed_sport_contents[x].items():
    numOfWordsForArticle = dict.fromkeys(uniqueWords, 0)
    for word in v.split(' '):
      numOfWordsForArticle[word] += 1
    numOfWords.update({k: numOfWordsForArticle})
    special_class.append(0)

tfs = {}
for k, v in numOfWords.items():
  tfs.update({k: computeTF(numOfWords[k], bagOfWords[k], len(bagOfWords[k]))})

idfs = computeIDF([v for k,v in numOfWords.items()])

tfids = {}
for k, v in tfs.items():
  tfids.update({k: computeTFIDF(tfs[k], idfs)})

import pandas as pd

data_frame_for_tf = pd.DataFrame([v for k,v in tfs.items()])
data_frame_for_tfid = pd.DataFrame([v for k,v in tfids.items()])

print(data_frame_for_tf)
print(data_frame_for_tfid)

import numpy as np
np.savetxt('termFrequency.txt', data_frame_for_tf.values[:100], fmt='%f', header =  "\t\t".join(data_frame_for_tf.columns))
np.savetxt('termFrequencyIDF.txt', data_frame_for_tf.values[:100], fmt='%f', header =  "\t\t".join(data_frame_for_tf.columns))

"""# Naive Bayes Classifier"""

from nltk import ngrams
from nltk.classify import NaiveBayesClassifier

def create_ngram_features(words, n):
    ngram_vocab = ngrams(words, n)
    my_dict = dict([(ng, True) for ng in ngram_vocab])
    return my_dict

for n in [1,2,3]:
  train_education_data = []
  for x in range(len(preprocessed_education_contents)):
    for k, v in preprocessed_education_contents[x].items():
      words = v.split(' ')
      train_education_data.append((create_ngram_features(words, n), "education"))
  
  train_sport_data = []
  for x in range(len(preprocessed_sport_contents)):
    for k, v in preprocessed_sport_contents[x].items():
       words = v.split(' ')
       train_sport_data.append((create_ngram_features(words, n), "sport"))

  test_education_data = []
  for x in range(len(preprocessed_education_contents)):
    for k, v in preprocessed_education_contents[x].items():
      words = v.split(' ')
      test_education_data.append((create_ngram_features(words, n), "education"))
  
  test_sport_data = []
  for x in range(len(preprocessed_sport_contents)):
    for k, v in preprocessed_sport_contents[x].items():
       words = v.split(' ')
       test_sport_data.append((create_ngram_features(words, n), "sport"))

  train_set = train_education_data[:946] + train_sport_data[:946]
  test_set =  test_education_data[:40] + test_sport_data[:40]

  classifier = NaiveBayesClassifier.train(train_set)
  
  true_negative = 0
  false_negative = 0
  true_positive = 0
  false_positive = 0

  results = classifier.classify_many([fs for (fs, l) in test_set])
  for ((fs, l), r) in zip(test_set, results):
    if l == r:
      if l == 'sport':
        true_negative = true_negative + 1 
      if l == 'education':
        true_positive = true_positive + 1
    if l != r:
      if l == 'sport':
        false_negative = false_negative + 1 
      if l == 'education':
        false_positive = false_positive + 1
 
  accuracy = (true_negative+true_positive)/(false_negative+false_positive+true_negative+true_positive)
  recall = (true_positive/(true_positive+false_negative))
  precision = (true_positive/(true_positive+false_positive))
  specificity = (true_negative/(true_negative+false_positive))
  
  print("#### ",n,"-gram  ####")
  print("TN: ",true_negative)
  print("FN: ",false_negative)
  print("TP: ",true_positive)
  print("FP: ",false_positive)
  print("Accuracy: ", accuracy)
  print("Recall/Sensivity: ", recall)
  print("Precision: ", precision)
  print("Specificity: ", specificity)
  print("F1-Score: ", 2*((precision*recall)/(precision+recall)))
  print("")

X = np.asarray(data_frame_for_tf)
y = np.asarray(special_class)

import tensorflow.python.keras
from tensorflow.python.keras import Sequential
from tensorflow.python.keras.layers import InputLayer
from tensorflow.python.keras.layers import Dense
from tensorflow.python.keras.layers import Dropout
from tensorflow.python.keras.constraints import maxnorm
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.2, random_state= 1234)

model = Sequential()
model.add(Dense(1024, input_dim=18098, activation='relu', kernel_constraint=maxnorm(3)))
model.add(Dropout(rate=0.2))
model.add(Dense(512, activation='relu', kernel_constraint=maxnorm(5)))
model.add(Dropout(rate=0.2))
model.add(Dense(256, activation='relu', kernel_constraint=maxnorm(5)))
model.add(Dropout(rate=0.2))
model.add(Dense(128, activation='relu', kernel_constraint=maxnorm(5)))
model.add(Dropout(rate=0.2))
model.add(Dense(64, activation='relu', kernel_constraint=maxnorm(5)))
model.add(Dropout(rate=0.2))
model.add(Dense(32, activation='relu', kernel_constraint=maxnorm(5)))
model.add(Dropout(rate=0.2))
model.add(Dense(16, activation='relu', kernel_constraint=maxnorm(5)))
model.add(Dropout(rate=0.2))
model.add(Dense(1, activation='sigmoid'))

model.compile(loss = "binary_crossentropy", optimizer = 'adam', metrics=['accuracy'])

train_acc = model.evaluate(X_train, y_train, verbose=0)
test_acc = model.evaluate(X_test, y_test, verbose=0)

history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=40, batch_size=50)