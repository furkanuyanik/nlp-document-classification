{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/furkanuyanik/nlp-document-classification/blob/master/NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "529zTbmfTDhJ",
        "colab_type": "text"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-9JSHrdRZh9",
        "colab_type": "code",
        "outputId": "3a69a2f5-b38d-44bb-f328-36fbe5b0396e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "import glob\n",
        "import os\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer \n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import wordnet\n",
        "nltk.download('stopwords');\n",
        "nltk.download('punkt');\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GsFjmF1UrxC",
        "colab_type": "text"
      },
      "source": [
        "# Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGj9ZS-tX7cB",
        "colab_type": "text"
      },
      "source": [
        "File IO Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8D1kFfOqX70h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read text files\n",
        "def readFiles(file_names):\n",
        "  raw_articles = [];\n",
        "\n",
        "  for file_name in file_names:\n",
        "    f = open(file_name, \"r\");\n",
        "    if f.mode == 'r':\n",
        "      try:\n",
        "        raw_articles.append({file_name: f.read()});\n",
        "        preprocessed_file_names.append(file_name)\n",
        "      except: \n",
        "        not_preprocessed_file_names.append(file_name)\n",
        "\n",
        "  return raw_articles;\n",
        "\n",
        "# Create directory by path\n",
        "def create_directory(directory_name):\n",
        "  if not os.path.exists(directory_name):\n",
        "    os.mkdir(directory_name)\n",
        "    print(\"Directory \", directory_name,  \" created.\")\n",
        "  else:    \n",
        "    print(\"Directory \", directory_name,  \" already exists.\")\n",
        "\n",
        "# Write content in the text file\n",
        "def writeFile(file_name, content):\n",
        "  f = open(file_name, \"w+\")\n",
        "  f.write(content)\n",
        "  f.close()\n",
        "\n",
        "# Get text file names\n",
        "def getFileNames(path):\n",
        "  return glob.glob(path + \"/*.txt\");\n",
        "\n",
        "# Get stop-words\n",
        "def getStopWords():\n",
        "  return set(stopwords.words('english'));"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CoUxZQXvVeKx",
        "colab_type": "text"
      },
      "source": [
        "Preprocessing Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvEVoRTLVl1h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get splitted words\n",
        "def getTokens(text):\n",
        "  return word_tokenize(text.lower());\n",
        "\n",
        "# Remove stop-word \n",
        "def removeStopWords(text):\n",
        "  temp = [w for w in getTokens(text) if not w in stopwords.words()]\n",
        "  return \" \".join(temp)\n",
        "\n",
        "# Remove duplicated words \n",
        "def removeDuplicatedWords(text):\n",
        "    temp = []\n",
        "    [temp.append(' '+w+' ') for w in getTokens(text) if (' '+w+' ') not in temp]\n",
        "    return \" \".join(getTokens(\"\".join(temp)))\n",
        "\n",
        "# Get root of words\n",
        "def getStemmingWords(text):\n",
        "  temp = [];\n",
        "  porter_stemmer = PorterStemmer() \n",
        "  for w in getTokens(text): \n",
        "    temp.append(porter_stemmer.stem(w));\n",
        "  return \" \".join(temp)\n",
        "\n",
        "# Remove special characters in the words\n",
        "def removeSpecialCharacters(text):\n",
        "  temp = list(text) \n",
        "  for x in range(len(text)):\n",
        "    if text[x] != ' ' and (text[x].isalnum() == False or text[x].isdigit() == True):\n",
        "        temp[x] = ' '\n",
        "\n",
        "  return \"\".join(temp)\n",
        "\n",
        "def removeSynonymWords(text):\n",
        "  temp = []\n",
        "  for word in getTokens(text):\n",
        "    for synset in wordnet.synsets(word):\n",
        "      for lemma in synset.lemma_names():\n",
        "          if lemma.lower() != word and lemma.find(\"_\") == -1 and not lemma in temp and text.find(' ' + lemma + ' ') != -1:\n",
        "            text = text.replace(' ' + lemma + ' ',' ')\n",
        "            temp.append(lemma)\n",
        "            temp.append(word)\n",
        "  return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZL7NFs82TAx5",
        "colab_type": "text"
      },
      "source": [
        "# Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svi4teXrR28W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stop_words =  getStopWords();\n",
        "\n",
        "preprocessed_file_names = []\n",
        "not_preprocessed_file_names = []\n",
        "\n",
        "train_education_file_names = getFileNames(\"train/raw/education\")\n",
        "train_sport_file_names = getFileNames(\"train/raw/sport\")\n",
        "test_education_file_names = getFileNames(\"test/raw/education\")\n",
        "test_sport_file_names = getFileNames(\"test/raw/sport\")\n",
        "\n",
        "train_education_contents = readFiles(train_education_file_names)\n",
        "train_sport_contents = readFiles(train_sport_file_names)\n",
        "test_education_contents = readFiles(test_education_file_names)\n",
        "test_sport_contents = readFiles(test_sport_file_names)\n",
        "\n",
        "preprocessed_directory_name = \"train/preprocessed\"\n",
        "preprocessed_education_directory_name = \"train/preprocessed/education\"\n",
        "preprocessed_sport_directory_name = \"train/preprocessed/sport\"\n",
        "preprocessed_test_directory_name = \"test/preprocessed\"\n",
        "preprocessed_test_education_directory_name = \"test/preprocessed/education\"\n",
        "preprocessed_test_sport_directory_name = \"test/preprocessed/sport\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6YMxsmlW1MC",
        "colab_type": "text"
      },
      "source": [
        "# Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hq3G53DxX6W0",
        "colab_type": "code",
        "outputId": "7b5feb64-6bcc-4828-ff73-95dfea5b2670",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# Create directory to save for preprocessed files\n",
        "create_directory(preprocessed_directory_name)\n",
        "create_directory(preprocessed_education_directory_name)\n",
        "create_directory(preprocessed_sport_directory_name)\n",
        "\n",
        "# Preprocess for education category\n",
        "for x in range(len(train_education_contents)):\n",
        "  for k,v in train_education_contents[x].items():\n",
        "    text_for_education = removeSpecialCharacters(v)\n",
        "    text_for_education = removeStopWords(text_for_education)\n",
        "    #text_for_education = removeDuplicatedWords(text_for_education)\n",
        "    text_for_education = removeSynonymWords(text_for_education)\n",
        "    text_for_education = getStemmingWords(text_for_education)\n",
        "    #text_for_education = removeDuplicatedWords(text_for_education)\n",
        "    file_path = preprocessed_education_directory_name + '/' + k[k.rindex('/') + 1:]\n",
        "    writeFile(file_path, text_for_education)\n",
        "\n",
        "print(\"Prepocessed completed for education category.\")\n",
        "\n",
        "# Preprocess for sport category\n",
        "for x in range(len(train_sport_contents)):\n",
        "  for k,v in train_sport_contents[x].items():\n",
        "    text_for_sport = removeSpecialCharacters(v)\n",
        "    text_for_sport = removeStopWords(text_for_sport)\n",
        "    #text_for_sport = removeDuplicatedWords(text_for_sport)\n",
        "    text_for_sport = removeSynonymWords(text_for_sport)\n",
        "    text_for_sport = getStemmingWords(text_for_sport)\n",
        "    #text_for_sport = removeDuplicatedWords(text_for_sport)\n",
        "    file_path = preprocessed_sport_directory_name + '/' + k[k.rindex('/') + 1:]\n",
        "    writeFile(file_path, text_for_sport)\n",
        "\n",
        "print(\"Prepocessed completed for sport category.\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Directory  train/preprocessed  created.\n",
            "Directory  train/preprocessed/education  created.\n",
            "Directory  train/preprocessed/sport  created.\n",
            "Prepocessed completed for education category.\n",
            "Prepocessed completed for sport category.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmuWL11Ya_C3",
        "colab_type": "code",
        "outputId": "c7b2b1d0-9a41-44f0-8e35-e1e584350202",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# Create directory to save for preprocessed files\n",
        "create_directory(preprocessed_test_directory_name)\n",
        "create_directory(preprocessed_test_education_directory_name)\n",
        "create_directory(preprocessed_test_sport_directory_name)\n",
        "\n",
        "# Preprocess for education category\n",
        "for x in range(len(test_education_contents)):\n",
        "  for k,v in test_education_contents[x].items():\n",
        "    text_for_education = removeSpecialCharacters(v)\n",
        "    text_for_education = removeStopWords(text_for_education)\n",
        "    #text_for_education = removeDuplicatedWords(text_for_education)\n",
        "    text_for_education = removeSynonymWords(text_for_education)\n",
        "    text_for_education = getStemmingWords(text_for_education)\n",
        "    #text_for_education = removeDuplicatedWords(text_for_education)\n",
        "    file_path = preprocessed_test_education_directory_name + '/' + k[k.rindex('/') + 1:]\n",
        "    writeFile(file_path, text_for_education)\n",
        "\n",
        "print(\"Prepocessed completed for education category.\")\n",
        "\n",
        "# Preprocess for sport category\n",
        "for x in range(len(test_sport_contents)):\n",
        "  for k,v in test_sport_contents[x].items():\n",
        "    text_for_sport = removeSpecialCharacters(v)\n",
        "    text_for_sport = removeStopWords(text_for_sport)\n",
        "    #text_for_sport = removeDuplicatedWords(text_for_sport)\n",
        "    text_for_sport = removeSynonymWords(text_for_sport)\n",
        "    text_for_sport = getStemmingWords(text_for_sport)\n",
        "    #text_for_sport = removeDuplicatedWords(text_for_sport)\n",
        "    file_path = preprocessed_test_sport_directory_name + '/' + k[k.rindex('/') + 1:]\n",
        "    writeFile(file_path, text_for_sport)\n",
        "\n",
        "print(\"Prepocessed completed for sport category.\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Directory  test/preprocessed  created.\n",
            "Directory  test/preprocessed/education  created.\n",
            "Directory  test/preprocessed/sport  created.\n",
            "Prepocessed completed for education category.\n",
            "Prepocessed completed for sport category.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzREKWfWuWaa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download preprocessed files.\n",
        "!zip -r train/preprocessed.zip train/preprocessed\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"train/preprocessed.zip\")\n",
        "\n",
        "print(\"Preprocessed text files downloaded.\")\n",
        "\n",
        "# Download preprocessed files.\n",
        "!zip -r test/preprocessed.zip test/preprocessed\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"test/preprocessed.zip\")\n",
        "\n",
        "print(\"Preprocessed text files downloaded.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uqIZqAnxWpo",
        "colab_type": "text"
      },
      "source": [
        "# Methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQfXDUirLg8Y",
        "colab_type": "text"
      },
      "source": [
        "Prepaire for calculate probability"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DeHnpugLgQ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "from nltk import ConditionalFreqDist\n",
        "\n",
        "preprocessed_education_file_names = getFileNames(preprocessed_education_directory_name)\n",
        "preprocessed_sport_file_names = getFileNames(preprocessed_sport_directory_name)\n",
        "preprocessed_test_education_file_names = getFileNames(preprocessed_test_education_directory_name)\n",
        "preprocessed_test_sport_file_names = getFileNames(preprocessed_test_sport_directory_name)\n",
        "\n",
        "preprocessed_education_contents = readFiles(preprocessed_education_file_names)\n",
        "preprocessed_sport_contents = readFiles(preprocessed_sport_file_names)\n",
        "preprocessed_test_education_contents = readFiles(preprocessed_test_education_file_names)\n",
        "preprocessed_test_sport_contents = readFiles(preprocessed_test_sport_file_names)\n",
        "\n",
        "# Required for TF and TF-IDF\n",
        "bagOfWords = {}\n",
        "\n",
        "education_total_text = ''\n",
        "for x in range(len(preprocessed_education_contents)):\n",
        "  for k, v in preprocessed_education_contents[x].items():\n",
        "    education_total_text = education_total_text + ' ' + v\n",
        "    bagOfWords.update({k : v.split(' ')})\n",
        "\n",
        "sport_total_text = ''\n",
        "for x in range(len(preprocessed_sport_contents)):\n",
        "  for k, v in preprocessed_sport_contents[x].items():\n",
        "    sport_total_text = sport_total_text + ' ' + v\n",
        "    bagOfWords.update({k : v.split(' ')})\n",
        "\n",
        "total_text = education_total_text + ' ' + sport_total_text\n",
        "\n",
        "total_for_education = len(getTokens(education_total_text))\n",
        "total_for_sport = len(getTokens(sport_total_text))\n",
        "\n",
        "bigrams_for_education = nltk.bigrams(getTokens(education_total_text))\n",
        "bigrams_for_sport = nltk.bigrams(getTokens(sport_total_text))\n",
        "\n",
        "trigrams_for_education = nltk.trigrams(getTokens(education_total_text))\n",
        "trigrams_for_sport = nltk.trigrams(getTokens(sport_total_text))\n",
        "\n",
        "fd_unigrams_for_education = nltk.FreqDist(getTokens(education_total_text))\n",
        "fd_unigrams_for_sport = nltk.FreqDist(getTokens(sport_total_text))\n",
        "\n",
        "cfd_bigrams_for_education = nltk.ConditionalFreqDist(bigrams_for_education) \n",
        "cfd_bigrams_for_sport = nltk.ConditionalFreqDist(bigrams_for_sport) \n",
        "\n",
        "cfd_trigrams_for_education = nltk.ConditionalFreqDist(((w2, (w0, w1)) for w0, w1, w2 in trigrams_for_education))\n",
        "cfd_trigrams_for_sport = nltk.ConditionalFreqDist(((w2, (w0, w1)) for w0, w1, w2 in trigrams_for_sport))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGVEDGLZL4AI",
        "colab_type": "text"
      },
      "source": [
        "Probability Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YQ-SIsaL20D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For 1-gram; count(w1) + 1 / V (1-gram total count)\n",
        "# For 2-gram; count(w2|w1) + 1 / count(w1) + V (2-gram total count)\n",
        "# For 3-gram; count(w3|w1,w2) + 1 / count(w1,w2) + V (3-gram total count)\n",
        "def laplaceSmoothing(total_count, word_n_count, word_n1_count = 0):\n",
        "  return (word_n_count + 1) / (word_n1_count + total_count)\n",
        "\n",
        "def computeTF(wordDict, bagOfWords, wordsCount):\n",
        "    tfDict = {}\n",
        "    bagOfWordsCount = len(bagOfWords)\n",
        "    for word, count in wordDict.items():\n",
        "        tfDict[word] = (count + 1) / (float(bagOfWordsCount) + wordsCount)\n",
        "    return tfDict\n",
        "\n",
        "def computeIDF(documents):\n",
        "    import math\n",
        "    N = len(documents)\n",
        "    \n",
        "    idfDict = dict.fromkeys(documents[0].keys(), 0)\n",
        "    for document in documents:\n",
        "        for word, val in document.items():\n",
        "            if val > 0:\n",
        "                idfDict[word] += 1\n",
        "    \n",
        "    for word, val in idfDict.items():\n",
        "        idfDict[word] = math.log((N+1) / (float(val)+1)) + 1\n",
        "    return idfDict\n",
        "\n",
        "def computeTFIDF(tfBagOfWords, idfs):\n",
        "    tfidf = {}\n",
        "    for word, val in tfBagOfWords.items():\n",
        "        tfidf[word] = val * idfs[word]\n",
        "    return tfidf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMugVCyLDco3",
        "colab_type": "text"
      },
      "source": [
        "Compute 1-gram Probability for Education category"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cO_v-zccDfqu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "education_unigrams_probability = {}\n",
        "\n",
        "for k, v in fd_unigrams_for_education.items():\n",
        "  education_unigrams_probability.update({'P(' + k + ')': laplaceSmoothing(len(fd_unigrams_for_education.items()), v)})\n",
        "\n",
        "education_unigrams_probability = {k: v for k, v in sorted(education_unigrams_probability.items(), key=lambda item: item[1], reverse=True)}\n",
        "\n",
        "f = open(\"train/preprocessed/education_unigrams_probabilities.txt\",\"w+\")\n",
        "for k, v in education_unigrams_probability.items():\n",
        "  f.write(k+ \": %f\\r\\n\" % (v))\n",
        "f.close() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U--Ikq0mDH5k",
        "colab_type": "text"
      },
      "source": [
        "Compute 2-gram Probability for Education category"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_SkWpUPyyJx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "education_bigrams_probability = {}\n",
        "\n",
        "for key, value in cfd_bigrams_for_education.items():\n",
        "  for sub_key, sub_value in value.items():\n",
        "    education_bigrams_probability.update({'P(' + key + ', ' + sub_key + ')': laplaceSmoothing(len(cfd_bigrams_for_education.items()), cfd_bigrams_for_education[key][sub_key], fd_unigrams_for_education[sub_key])})\n",
        "\n",
        "education_bigrams_probability = {k: v for k, v in sorted(education_bigrams_probability.items(), key=lambda item: item[1], reverse=True)}\n",
        "\n",
        "f = open(\"train/preprocessed/education_bigrams_probabilities.txt\",\"w+\")\n",
        "for k, v in education_bigrams_probability.items():\n",
        "  f.write(k+ \": %f\\r\\n\" % (v))\n",
        "f.close() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEFHgtkyDhIk",
        "colab_type": "text"
      },
      "source": [
        "Compute 3-gram Probability for Education category"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhbORN6uDgni",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "education_trigrams_probability = {}\n",
        "\n",
        "for key, value in cfd_trigrams_for_education.items():\n",
        "  for sub_key, sub_value in value.items():\n",
        "    education_trigrams_probability.update({'P(' + key + ', ' + sub_key[0] + ', '+sub_key[1]+ ')' : laplaceSmoothing(len(cfd_trigrams_for_education.items()), cfd_trigrams_for_education[key][sub_key], cfd_bigrams_for_education[sub_key[1]][sub_key[0]])})\n",
        "\n",
        "education_trigrams_probability = {k: v for k, v in sorted(education_trigrams_probability.items(), key=lambda item: item[1], reverse=True)}\n",
        "\n",
        "f = open(\"train/preprocessed/education_trigrams_probabilities.txt\",\"w+\")\n",
        "for k, v in education_trigrams_probability.items():\n",
        "  f.write(k+ \": %f\\r\\n\" % (v))\n",
        "f.close() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0XwGBW4dCLBP"
      },
      "source": [
        "Compute 1-gram Probability for Sport category"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2a9_1F7wCLBR",
        "colab": {}
      },
      "source": [
        "sport_unigrams_probability = {}\n",
        "\n",
        "for k, v in fd_unigrams_for_sport.items():\n",
        "  sport_unigrams_probability.update({'P(' + k + ')': laplaceSmoothing(len(fd_unigrams_for_sport.items()), v)})\n",
        "\n",
        "sport_unigrams_probability = {k: v for k, v in sorted(sport_unigrams_probability.items(), key=lambda item: item[1], reverse=True)}\n",
        "\n",
        "f = open(\"train/preprocessed/sport_unigrams_probabilities.txt\",\"w+\")\n",
        "for k, v in sport_unigrams_probability.items():\n",
        "  f.write(k+ \": %f\\r\\n\" % (v))\n",
        "f.close() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YzET9SptCLBX"
      },
      "source": [
        "Compute 2-gram Probability for Sport category"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JmIeg6nlCLBY",
        "colab": {}
      },
      "source": [
        "sport_bigrams_probability = {}\n",
        "\n",
        "for key, value in cfd_bigrams_for_sport.items():\n",
        "  for sub_key, sub_value in value.items():\n",
        "    sport_bigrams_probability.update({'P(' + key + ', ' + sub_key + ')': laplaceSmoothing(len(cfd_bigrams_for_sport.items()), cfd_bigrams_for_sport[key][sub_key], fd_unigrams_for_sport[sub_key])})\n",
        "\n",
        "sport_bigrams_probability = {k: v for k, v in sorted(sport_bigrams_probability.items(), key=lambda item: item[1], reverse=True)}\n",
        "\n",
        "f = open(\"train/preprocessed/sport_bigrams_probabilities.txt\",\"w+\")\n",
        "for k, v in sport_bigrams_probability.items():\n",
        "  f.write(k+ \": %f\\r\\n\" % (v))\n",
        "f.close() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QiqoXjRLCLBb"
      },
      "source": [
        "Compute 3-gram Probability for Sport category"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oMH2hOZ4CLBc",
        "colab": {}
      },
      "source": [
        "sport_trigrams_probability = {}\n",
        "\n",
        "for key, value in cfd_trigrams_for_sport.items():\n",
        "  for sub_key, sub_value in value.items():\n",
        "    sport_trigrams_probability.update({'P(' + key + ', ' + sub_key[0] + ', '+sub_key[1]+ ')' : laplaceSmoothing(len(cfd_trigrams_for_sport.items()), cfd_trigrams_for_sport[key][sub_key], cfd_bigrams_for_sport[sub_key[1]][sub_key[0]])})\n",
        "\n",
        "sport_trigrams_probability = {k: v for k, v in sorted(sport_trigrams_probability.items(), key=lambda item: item[1], reverse=True)}\n",
        "\n",
        "f = open(\"train/preprocessed/sport_trigrams_probabilities.txt\",\"w+\")\n",
        "for k, v in sport_trigrams_probability.items():\n",
        "  f.write(k+ \": %f\\r\\n\" % (v))\n",
        "f.close() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adhbh79qXaZD",
        "colab_type": "text"
      },
      "source": [
        "Compute TF and TF-IDF Probability"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05LPGnq-X_V7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "uniqueWords = getTokens(removeDuplicatedWords(total_text))\n",
        "\n",
        "# For Artificial neural network (education: 1, sport: 0)\n",
        "special_class = []\n",
        "\n",
        "numOfWords = {}\n",
        "for x in range(len(preprocessed_education_contents)):\n",
        "  for k, v in preprocessed_education_contents[x].items():\n",
        "    numOfWordsForArticle = dict.fromkeys(uniqueWords, 0)\n",
        "    for word in v.split(' '):\n",
        "      numOfWordsForArticle[word] += 1\n",
        "    numOfWords.update({k: numOfWordsForArticle})\n",
        "    special_class.append(1)\n",
        "\n",
        "for x in range(len(preprocessed_sport_contents)):\n",
        "  for k, v in preprocessed_sport_contents[x].items():\n",
        "    numOfWordsForArticle = dict.fromkeys(uniqueWords, 0)\n",
        "    for word in v.split(' '):\n",
        "      numOfWordsForArticle[word] += 1\n",
        "    numOfWords.update({k: numOfWordsForArticle})\n",
        "    special_class.append(0)\n",
        "\n",
        "tfs = {}\n",
        "for k, v in numOfWords.items():\n",
        "  tfs.update({k: computeTF(numOfWords[k], bagOfWords[k], len(bagOfWords[k]))})\n",
        "\n",
        "idfs = computeIDF([v for k,v in numOfWords.items()])\n",
        "\n",
        "tfids = {}\n",
        "for k, v in tfs.items():\n",
        "  tfids.update({k: computeTFIDF(tfs[k], idfs)}) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bwgbeQXZmeM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "data_frame_for_tf = pd.DataFrame([v for k,v in tfs.items()])\n",
        "data_frame_for_tfid = pd.DataFrame([v for k,v in tfids.items()])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsfuUVxKlwy_",
        "colab_type": "code",
        "outputId": "7b1cc9c3-24f6-4f25-e45d-7121347e941c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "print(data_frame_for_tf)\n",
        "print(data_frame_for_tfid)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "        perdau    expand      choa  ...    carney  ethnosport     serik\n",
            "0     0.023148  0.018519  0.018519  ...  0.004630    0.004630  0.004630\n",
            "1     0.004950  0.004950  0.004950  ...  0.004950    0.004950  0.004950\n",
            "2     0.002232  0.002232  0.002232  ...  0.002232    0.002232  0.002232\n",
            "3     0.002165  0.002165  0.002165  ...  0.002165    0.002165  0.002165\n",
            "4     0.001016  0.001016  0.001016  ...  0.001016    0.001016  0.001016\n",
            "...        ...       ...       ...  ...       ...         ...       ...\n",
            "1026  0.002049  0.002049  0.002049  ...  0.002049    0.002049  0.002049\n",
            "1027  0.005208  0.005208  0.005208  ...  0.005208    0.005208  0.005208\n",
            "1028  0.002890  0.002890  0.002890  ...  0.002890    0.002890  0.002890\n",
            "1029  0.005556  0.005556  0.005556  ...  0.011111    0.005556  0.005556\n",
            "1030  0.002924  0.005848  0.002924  ...  0.002924    0.023392  0.005848\n",
            "\n",
            "[1031 rows x 18098 columns]\n",
            "        perdau    expand      choa  ...    carney  ethnosport     serik\n",
            "0     0.167734  0.067318  0.134187  ...  0.033547    0.033547  0.033547\n",
            "1     0.035872  0.017996  0.035872  ...  0.035872    0.035872  0.035872\n",
            "2     0.016174  0.008114  0.016174  ...  0.016174    0.016174  0.016174\n",
            "3     0.015684  0.007868  0.015684  ...  0.015684    0.015684  0.015684\n",
            "4     0.007364  0.003694  0.007364  ...  0.007364    0.007364  0.007364\n",
            "...        ...       ...       ...  ...       ...         ...       ...\n",
            "1026  0.014849  0.007449  0.014849  ...  0.014849    0.014849  0.014849\n",
            "1027  0.037740  0.018933  0.037740  ...  0.037740    0.037740  0.037740\n",
            "1028  0.020943  0.010506  0.020943  ...  0.020943    0.020943  0.020943\n",
            "1029  0.040256  0.020195  0.040256  ...  0.080512    0.040256  0.040256\n",
            "1030  0.021187  0.021258  0.021187  ...  0.021187    0.169500  0.042375\n",
            "\n",
            "[1031 rows x 18098 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0J-nnk7nGor",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "np.savetxt('termFrequency.txt', data_frame_for_tf.values[:100], fmt='%f', header =  \"\\t\\t\".join(data_frame_for_tf.columns))\n",
        "np.savetxt('termFrequencyIDF.txt', data_frame_for_tf.values[:100], fmt='%f', header =  \"\\t\\t\".join(data_frame_for_tf.columns))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6ZktPL1MfXT",
        "colab_type": "text"
      },
      "source": [
        "# Naive Bayes Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVmmxgZdNomA",
        "colab_type": "code",
        "outputId": "b76b68a1-9f2b-4a5b-9e33-b7653c74e6de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        }
      },
      "source": [
        "from nltk import ngrams\n",
        "from nltk.classify import NaiveBayesClassifier\n",
        "\n",
        "def create_ngram_features(words, n):\n",
        "    ngram_vocab = ngrams(words, n)\n",
        "    my_dict = dict([(ng, True) for ng in ngram_vocab])\n",
        "    return my_dict\n",
        "\n",
        "for n in [1,2,3]:\n",
        "  train_education_data = []\n",
        "  for x in range(len(preprocessed_education_contents)):\n",
        "    for k, v in preprocessed_education_contents[x].items():\n",
        "      words = v.split(' ')\n",
        "      train_education_data.append((create_ngram_features(words, n), \"education\"))\n",
        "  \n",
        "  train_sport_data = []\n",
        "  for x in range(len(preprocessed_sport_contents)):\n",
        "    for k, v in preprocessed_sport_contents[x].items():\n",
        "       words = v.split(' ')\n",
        "       train_sport_data.append((create_ngram_features(words, n), \"sport\"))\n",
        "\n",
        "  test_education_data = []\n",
        "  for x in range(len(preprocessed_education_contents)):\n",
        "    for k, v in preprocessed_education_contents[x].items():\n",
        "      words = v.split(' ')\n",
        "      test_education_data.append((create_ngram_features(words, n), \"education\"))\n",
        "  \n",
        "  test_sport_data = []\n",
        "  for x in range(len(preprocessed_sport_contents)):\n",
        "    for k, v in preprocessed_sport_contents[x].items():\n",
        "       words = v.split(' ')\n",
        "       test_sport_data.append((create_ngram_features(words, n), \"sport\"))\n",
        "\n",
        "  train_set = train_education_data[:946] + train_sport_data[:946]\n",
        "  test_set =  test_education_data[:40] + test_sport_data[:40]\n",
        "\n",
        "  classifier = NaiveBayesClassifier.train(train_set)\n",
        "  \n",
        "  true_negative = 0\n",
        "  false_negative = 0\n",
        "  true_positive = 0\n",
        "  false_positive = 0\n",
        "\n",
        "  results = classifier.classify_many([fs for (fs, l) in test_set])\n",
        "  for ((fs, l), r) in zip(test_set, results):\n",
        "    if l == r:\n",
        "      if l == 'sport':\n",
        "        true_negative = true_negative + 1 \n",
        "      if l == 'education':\n",
        "        true_positive = true_positive + 1\n",
        "    if l != r:\n",
        "      if l == 'sport':\n",
        "        false_negative = false_negative + 1 \n",
        "      if l == 'education':\n",
        "        false_positive = false_positive + 1\n",
        " \n",
        "  accuracy = (true_negative+true_positive)/(false_negative+false_positive+true_negative+true_positive)\n",
        "  recall = (true_positive/(true_positive+false_negative))\n",
        "  precision = (true_positive/(true_positive+false_positive))\n",
        "  specificity = (true_negative/(true_negative+false_positive))\n",
        "  \n",
        "  print(\"#### \",n,\"-gram  ####\")\n",
        "  print(\"TN: \",true_negative)\n",
        "  print(\"FN: \",false_negative)\n",
        "  print(\"TP: \",true_positive)\n",
        "  print(\"FP: \",false_positive)\n",
        "  print(\"Accuracy: \", accuracy)\n",
        "  print(\"Recall/Sensivity: \", recall)\n",
        "  print(\"Precision: \", precision)\n",
        "  print(\"Specificity: \", specificity)\n",
        "  print(\"F1-Score: \", 2*((precision*recall)/(precision+recall)))\n",
        "  print(\"\")"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "####  1 -gram  ####\n",
            "TN:  40\n",
            "FN:  0\n",
            "TP:  39\n",
            "FP:  1\n",
            "Accuracy:  0.9875\n",
            "Recall/Sensivity:  1.0\n",
            "Precision:  0.975\n",
            "Specificity:  0.975609756097561\n",
            "F1-Score:  0.9873417721518987\n",
            "\n",
            "####  2 -gram  ####\n",
            "TN:  40\n",
            "FN:  0\n",
            "TP:  40\n",
            "FP:  0\n",
            "Accuracy:  1.0\n",
            "Recall/Sensivity:  1.0\n",
            "Precision:  1.0\n",
            "Specificity:  1.0\n",
            "F1-Score:  1.0\n",
            "\n",
            "####  3 -gram  ####\n",
            "TN:  40\n",
            "FN:  0\n",
            "TP:  40\n",
            "FP:  0\n",
            "Accuracy:  1.0\n",
            "Recall/Sensivity:  1.0\n",
            "Precision:  1.0\n",
            "Specificity:  1.0\n",
            "F1-Score:  1.0\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "na1jsXpQp8_M",
        "colab_type": "code",
        "outputId": "7624c26e-eb5f-4f81-b268-a4bd56ca994e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "X = np.asarray(data_frame_for_tf)\n",
        "y = np.asarray(special_class)\n",
        "\n",
        "import tensorflow.python.keras\n",
        "from tensorflow.python.keras import Sequential\n",
        "from tensorflow.python.keras.layers import InputLayer\n",
        "from tensorflow.python.keras.layers import Dense\n",
        "from tensorflow.python.keras.layers import Dropout\n",
        "from tensorflow.python.keras.constraints import maxnorm\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.2, random_state= 1234)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(1024, input_dim=18098, activation='relu', kernel_constraint=maxnorm(3)))\n",
        "model.add(Dropout(rate=0.2))\n",
        "model.add(Dense(512, activation='relu', kernel_constraint=maxnorm(5)))\n",
        "model.add(Dropout(rate=0.2))\n",
        "model.add(Dense(256, activation='relu', kernel_constraint=maxnorm(5)))\n",
        "model.add(Dropout(rate=0.2))\n",
        "model.add(Dense(128, activation='relu', kernel_constraint=maxnorm(5)))\n",
        "model.add(Dropout(rate=0.2))\n",
        "model.add(Dense(64, activation='relu', kernel_constraint=maxnorm(5)))\n",
        "model.add(Dropout(rate=0.2))\n",
        "model.add(Dense(32, activation='relu', kernel_constraint=maxnorm(5)))\n",
        "model.add(Dropout(rate=0.2))\n",
        "model.add(Dense(16, activation='relu', kernel_constraint=maxnorm(5)))\n",
        "model.add(Dropout(rate=0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss = \"binary_crossentropy\", optimizer = 'adam', metrics=['accuracy'])\n",
        "\n",
        "train_acc = model.evaluate(X_train, y_train, verbose=0)\n",
        "test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=40, batch_size=50)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "17/17 [==============================] - 4s 218ms/step - loss: 0.6978 - accuracy: 0.4988 - val_loss: 0.6925 - val_accuracy: 0.5169\n",
            "Epoch 2/40\n",
            "17/17 [==============================] - 4s 218ms/step - loss: 0.6964 - accuracy: 0.5121 - val_loss: 0.6888 - val_accuracy: 0.5652\n",
            "Epoch 3/40\n",
            "17/17 [==============================] - 4s 218ms/step - loss: 0.6953 - accuracy: 0.5316 - val_loss: 0.6884 - val_accuracy: 0.5797\n",
            "Epoch 4/40\n",
            "17/17 [==============================] - 4s 218ms/step - loss: 0.6888 - accuracy: 0.5607 - val_loss: 0.6888 - val_accuracy: 0.6087\n",
            "Epoch 5/40\n",
            "17/17 [==============================] - 4s 218ms/step - loss: 0.6933 - accuracy: 0.5255 - val_loss: 0.6854 - val_accuracy: 0.6135\n",
            "Epoch 6/40\n",
            "17/17 [==============================] - 4s 217ms/step - loss: 0.6845 - accuracy: 0.5534 - val_loss: 0.6822 - val_accuracy: 0.6280\n",
            "Epoch 7/40\n",
            "17/17 [==============================] - 4s 216ms/step - loss: 0.6866 - accuracy: 0.5510 - val_loss: 0.6780 - val_accuracy: 0.6135\n",
            "Epoch 8/40\n",
            "17/17 [==============================] - 4s 217ms/step - loss: 0.6843 - accuracy: 0.5692 - val_loss: 0.6798 - val_accuracy: 0.6280\n",
            "Epoch 9/40\n",
            "17/17 [==============================] - 4s 216ms/step - loss: 0.6928 - accuracy: 0.5667 - val_loss: 0.6716 - val_accuracy: 0.5749\n",
            "Epoch 10/40\n",
            "17/17 [==============================] - 4s 216ms/step - loss: 0.6829 - accuracy: 0.5692 - val_loss: 0.6708 - val_accuracy: 0.6232\n",
            "Epoch 11/40\n",
            "17/17 [==============================] - 4s 221ms/step - loss: 0.6814 - accuracy: 0.5570 - val_loss: 0.6662 - val_accuracy: 0.6184\n",
            "Epoch 12/40\n",
            "17/17 [==============================] - 4s 218ms/step - loss: 0.6730 - accuracy: 0.5728 - val_loss: 0.6576 - val_accuracy: 0.6329\n",
            "Epoch 13/40\n",
            "17/17 [==============================] - 4s 219ms/step - loss: 0.6592 - accuracy: 0.6153 - val_loss: 0.6380 - val_accuracy: 0.6473\n",
            "Epoch 14/40\n",
            "17/17 [==============================] - 4s 219ms/step - loss: 0.6401 - accuracy: 0.6262 - val_loss: 0.6844 - val_accuracy: 0.5121\n",
            "Epoch 15/40\n",
            "17/17 [==============================] - 4s 217ms/step - loss: 0.6158 - accuracy: 0.6650 - val_loss: 0.5446 - val_accuracy: 0.7440\n",
            "Epoch 16/40\n",
            "17/17 [==============================] - 4s 219ms/step - loss: 0.5096 - accuracy: 0.7658 - val_loss: 0.4086 - val_accuracy: 0.8261\n",
            "Epoch 17/40\n",
            "17/17 [==============================] - 4s 217ms/step - loss: 0.5175 - accuracy: 0.7415 - val_loss: 0.5563 - val_accuracy: 0.6763\n",
            "Epoch 18/40\n",
            "17/17 [==============================] - 4s 217ms/step - loss: 0.5936 - accuracy: 0.7015 - val_loss: 0.4900 - val_accuracy: 0.7874\n",
            "Epoch 19/40\n",
            "17/17 [==============================] - 4s 217ms/step - loss: 0.5284 - accuracy: 0.7488 - val_loss: 0.4866 - val_accuracy: 0.7971\n",
            "Epoch 20/40\n",
            "17/17 [==============================] - 4s 208ms/step - loss: 0.4520 - accuracy: 0.8325 - val_loss: 0.4414 - val_accuracy: 0.7923\n",
            "Epoch 21/40\n",
            "17/17 [==============================] - 4s 208ms/step - loss: 0.4476 - accuracy: 0.8228 - val_loss: 0.3874 - val_accuracy: 0.8454\n",
            "Epoch 22/40\n",
            "17/17 [==============================] - 4s 210ms/step - loss: 0.5009 - accuracy: 0.7864 - val_loss: 0.3775 - val_accuracy: 0.8792\n",
            "Epoch 23/40\n",
            "17/17 [==============================] - 4s 209ms/step - loss: 0.4146 - accuracy: 0.8350 - val_loss: 0.4772 - val_accuracy: 0.7729\n",
            "Epoch 24/40\n",
            "17/17 [==============================] - 4s 209ms/step - loss: 0.4288 - accuracy: 0.8277 - val_loss: 0.4943 - val_accuracy: 0.7729\n",
            "Epoch 25/40\n",
            "17/17 [==============================] - 4s 209ms/step - loss: 0.3951 - accuracy: 0.8507 - val_loss: 0.2627 - val_accuracy: 0.9179\n",
            "Epoch 26/40\n",
            "17/17 [==============================] - 4s 207ms/step - loss: 0.3928 - accuracy: 0.8483 - val_loss: 0.5166 - val_accuracy: 0.7633\n",
            "Epoch 27/40\n",
            "17/17 [==============================] - 4s 211ms/step - loss: 0.4344 - accuracy: 0.8216 - val_loss: 0.4328 - val_accuracy: 0.8116\n",
            "Epoch 28/40\n",
            "17/17 [==============================] - 4s 209ms/step - loss: 0.3771 - accuracy: 0.8568 - val_loss: 0.4281 - val_accuracy: 0.8116\n",
            "Epoch 29/40\n",
            "17/17 [==============================] - 4s 209ms/step - loss: 0.5200 - accuracy: 0.7803 - val_loss: 0.3024 - val_accuracy: 0.9565\n",
            "Epoch 30/40\n",
            "17/17 [==============================] - 4s 208ms/step - loss: 0.6070 - accuracy: 0.7354 - val_loss: 0.5280 - val_accuracy: 0.7874\n",
            "Epoch 31/40\n",
            "17/17 [==============================] - 4s 207ms/step - loss: 0.5150 - accuracy: 0.7840 - val_loss: 0.5183 - val_accuracy: 0.7729\n",
            "Epoch 32/40\n",
            "17/17 [==============================] - 3s 205ms/step - loss: 0.4967 - accuracy: 0.7900 - val_loss: 0.5179 - val_accuracy: 0.7488\n",
            "Epoch 33/40\n",
            "17/17 [==============================] - 4s 207ms/step - loss: 0.4483 - accuracy: 0.8240 - val_loss: 0.3618 - val_accuracy: 0.8792\n",
            "Epoch 34/40\n",
            "17/17 [==============================] - 3s 203ms/step - loss: 0.3885 - accuracy: 0.8677 - val_loss: 0.3579 - val_accuracy: 0.8792\n",
            "Epoch 35/40\n",
            "17/17 [==============================] - 3s 202ms/step - loss: 0.4805 - accuracy: 0.8010 - val_loss: 0.6967 - val_accuracy: 0.6908\n",
            "Epoch 36/40\n",
            "17/17 [==============================] - 4s 207ms/step - loss: 0.3764 - accuracy: 0.8689 - val_loss: 0.2386 - val_accuracy: 0.9710\n",
            "Epoch 37/40\n",
            "17/17 [==============================] - 4s 209ms/step - loss: 0.3529 - accuracy: 0.8786 - val_loss: 0.5521 - val_accuracy: 0.8309\n",
            "Epoch 38/40\n",
            "17/17 [==============================] - 4s 211ms/step - loss: 0.3859 - accuracy: 0.8592 - val_loss: 0.2130 - val_accuracy: 0.9662\n",
            "Epoch 39/40\n",
            "17/17 [==============================] - 4s 207ms/step - loss: 0.3123 - accuracy: 0.8981 - val_loss: 0.2004 - val_accuracy: 0.9614\n",
            "Epoch 40/40\n",
            "17/17 [==============================] - 4s 209ms/step - loss: 0.2918 - accuracy: 0.9102 - val_loss: 0.4572 - val_accuracy: 0.8019\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}